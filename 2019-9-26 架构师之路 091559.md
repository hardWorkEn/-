# 架构师之路

## 1、秒杀系统优化思路

### **一、优化方向**

优化有2个方面：

①**将请求尽量拦截在系统上游**（不要让锁冲突落到数据库中）。传统秒杀系统之所以挂，请求都压到了后端数据层，数据读写锁冲突严重，并发高响应慢，几乎所有请求都超时，流量虽大，下单成功的几乎没有。

②**充分利用缓存**，秒杀买票，这是一个典型的读多写少的应用场景，大部分请求是车次查询，票查询，下单和支付才是写请求。

### **二、常见秒杀架构**

常见的站点架构基本是这样的

![1569382121375](C:\Users\RYQBDYQ\AppData\Local\Temp\1569382121375.png)

（1）浏览器端：最上层，会执行一些js代码

（2）站点层：这一层会访问后端数据，拼html页面返回给浏览器

（3）服务层：向上游屏蔽底层数据细节，提供数据访问

（4）数据层：最终的库存是存在这里的。

### **三、各层次优化细节**

#### **第一层，客户端如何优化（浏览器层，APP层）**

问大家一个问题，大家都玩过微信的摇一摇抢红包对吧，每次摇一摇，就会往后端发送请求么？回顾我们下单抢票的场景，点击了“查询”按钮之后，系统那个卡呀，进度条涨的慢呀，作为用户，我会不自觉的再去点击“查询”，对么？继续点，继续点，点点点。。。有用么？平白无故的增加了系统负载，一个用户点5次，80%的请求是这么多出来的，怎么整？ 

（a）**产品层面**，用户点击查询或者购票后，按钮置灰，禁止用户重复提交请求。

（b）**JS层面**，限制用户在X秒之内只能提交一次请求。

APP层面，可以做类似的事情，虽然你疯狂的在摇微信，其实x秒才向后端发起一次请求。这就是所谓的“将请求尽量拦截在系统上游”，越上游越好，浏览器层，APP层就给拦住，这样就能挡住80%+的请求，这种办法只能拦住普通用户（但99%的用户是普通用户）对于群内的高端程序员是拦不住的。firebug一抓包，http长啥样都知道，js是万万拦不住程序员写for循环，调用http接口的，这部分请求怎么处理？ 

#### **第二层，站点层面的请求拦截**

怎么拦截？怎么防止程序员写for循环调用，有去重依据么？ip？cookie-id？…想复杂了，这类业务都需要登录，用uid即可。在站点层面，对uid进行请求计数和去重，甚至不需要统一存储计数，直接站点层内存存储（这样计数会不准，但最简单）。一个uid，5秒只准透过1个请求，这样又能拦住99%的for循环请求。

5s只透过一个请求，其余的请求怎么办？缓存，页面缓存，同一个uid，限制访问频度，做页面缓存，x秒内到达站点层的请求，均返回同一页面。同一个item的查询，例如车次，做页面缓存，x秒内到达站点层的请求，均返回同一页面。如此限流，既能保证用户有良好的用户体验（没有返回404）又能保证系统的健壮性（利用页面缓存，把请求拦截在站点层了）。

页面缓存不一定要保证所有站点返回一致的页面，直接放在每个站点的内存也是可以的。优点是简单，坏处是http请求落到不同的站点，返回的车票数据可能不一样，这是站点层的请求拦截与缓存优化。

好，这个方式拦住了写for循环发http请求的程序员，有些高端程序员（黑客）控制了10w个肉鸡，手里有10w个uid，同时发请求（先不考虑实名制的问题，小米抢手机不需要实名制），这下怎么办，站点层按照uid限流拦不住了。

#### **第三层，服务层来拦截**

服务层怎么拦截？大哥，我是服务层，我清楚的知道小米只有1万部手机，我清楚的知道一列火车只有2000张车票，我透10w个请求去数据库有什么意义呢？没错，**请求队列！**

对于写请求，做请求队列，每次只透有限的写请求去数据层（下订单，支付这样的写业务）

1w部手机，只透1w个下单请求去db

3k张火车票，只透3k个下单请求去db

如果均成功再放下一批，如果库存不够则队列里的写请求全部返回“已售完”。

对于读请求，怎么优化？cache抗，不管是memcached还是redis，单机抗个每秒10w应该都是没什么问题的。如此限流，只有非常少的写请求，和非常少的读缓存mis的请求会透到数据层去，又有99.9%的请求被拦住了。

当然，还有业务规则上的一些优化。回想12306所做的，分时分段售票，原来统一10点卖票，现在8点，8点半，9点，...每隔半个小时放出一批：将流量摊匀。

其次，数据粒度的优化：你去购票，对于余票查询这个业务，票剩了58张，还是26张，你真的关注么，其实我们只关心有票和无票？流量大的时候，做一个粗粒度的“有票”“无票”缓存即可。

第三，一些业务逻辑的异步：例如下单业务与 支付业务的分离。这些优化都是结合 业务 来的，我之前分享过一个观点“**一切脱离业务的架构设计都是耍流氓**”架构的优化也要针对业务。

#### **第四层，最后是数据库层**

浏览器拦截了80%，站点层拦截了99.9%并做了页面缓存，服务层又做了写请求队列与数据缓存，每次透到数据库层的请求都是可控的。db基本就没什么压力了，闲庭信步，单机也能扛得住，还是那句话，库存是有限的，小米的产能有限，透这么多请求来数据库没有意义。

全部透到数据库，100w个下单，0个成功，请求有效率0%。透3k个到数据，全部成功，请求有效率100%。

### **四、总结**

上文应该描述的非常清楚了，没什么总结了，对于秒杀系统，再次重复下我个人经验的两个架构优化思路：

（1）尽量将请求拦截在系统上游（越上游越好）；

（2）**读多写少的常用多使用缓存**（缓存抗读压力）；

浏览器和APP：做限速

站点层：按照uid做限速，做页面缓存

服务层：按照业务做写请求队列控制流量，做数据缓存

数据层：闲庭信步

并且：结合业务做优化

## 2、细聊分布式ID生成方法

### 一、需求缘起

几乎所有的业务系统，都有生成一个记录标识的需求，例如：

 （1）消息标识：message-id

 （2）订单标识：order-id

 （3）帖子标识：tiezi-id

这个记录标识往往就是数据库中的**唯一主键**，数据库上会建立聚集索引（cluster index），即在物理存储上以这个字段排序。这个记录标识上的查询，往往又有分页或者排序的业务需求，例如：

 （1）拉取最新的一页消息：selectmessage-id/ order by time/ limit 100

 （2）拉取最新的一页订单：selectorder-id/ order by time/ limit 100

 （3）拉取最新的一页帖子：selecttiezi-id/ order by time/ limit 100

所以往往要有一个time字段，并且在time字段上建立普通索引（non-cluster index）。

 我们都知道普通索引存储的是实际记录的指针，其访问效率会比聚集索引慢，如果记录标识在生成时能够基本按照时间有序，则可以省去这个time字段的索引查询：

 select message-id/ (order by message-id)/limit 100

再次强调，能这么做的前提是，message-id的生成基本是**趋势时间递增的**。

 这就引出了记录标识生成（也就是上文提到的三个XXX-id）的两大核心需求：

 （1）全局唯一

 （2）趋势有序

这也是本文要讨论的核心问题：**如何高效生成趋势有序的全局唯一ID**。

### 二、常见方法、不足与优化

【常见方法一：使用数据库的auto_increment来生成全局唯一递增ID】

**优点：**

（1）简单，使用数据库已有的功能

（2）能够保证唯一性

（3）能够保证递增性

（4）步长固定

**缺点：**

（1）可用性难以保证，数据库常见架构是一主多从+读写分离，生成自增ID是写请求，主库挂了就玩不转了。

（2）扩展性差，性能有上限，因为写入是单点，数据库主库的写性能决定ID的生成性能上限，并且难以扩展。

**改进方法：**

（1）增加主库，避免写入单点

（2）数据水平切分，保证各主库生成的ID不重复

![1569393221170](C:\Users\RYQBDYQ\AppData\Local\Temp\1569393221170.png)

如上图所述，由1个写库变成3个写库，每个写库设置不同的auto_increment初始值，以及相同的增长步长，以保证每个数据库生成的ID是不同的。改进后的架构保证了可用性，但缺点是：

（1）丧失了ID生成的绝对递增性：先访问库0生成0，3，在访问库1生成1，可能导致在非常短的时间内，ID生成不是绝对递增的。

（2）数据库的写压力依然很大，每次生成ID都要访问数据库。

【常见方法二：单点批量ID生成服务】

